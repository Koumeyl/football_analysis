{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def extract_frame(video_path, frame_number, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(output_path, frame)\n",
    "    cap.release()\n",
    "    return output_path\n",
    "\n",
    "# Example usage\n",
    "video_path = 'input_videos/antwerp_angle.mp4'\n",
    "frame_number = 250\n",
    "output_image_path = 'dataset_maker/frame_250.jpg'\n",
    "extract_frame(video_path, frame_number, output_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model_path = 'models/Field_Key_Points.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = YOLO(model_path).to(device)\n",
    "\n",
    "def predict_keypoints(image_path, model):\n",
    "    # Predict keypoints using the YOLO model\n",
    "    results = model.predict(image_path, device=device, conf=0.5)\n",
    "    \n",
    "    # Extract results from the prediction\n",
    "    result = results[0]\n",
    "    \n",
    "    # Extract bounding box (normalized)\n",
    "    if result.boxes:\n",
    "        bbox = result.boxes.xywhn[0].tolist()  # xywh normalized format and convert to list of floats\n",
    "    else:\n",
    "        bbox = [0, 0, 0, 0]\n",
    "    \n",
    "    # Extract keypoints and confidences\n",
    "    keypoints = result.keypoints.xyn[0].tolist()  # Normalized keypoints and convert to list of lists\n",
    "    confidences = result.keypoints.conf[0].tolist()  # Confidence scores and convert to list\n",
    "    \n",
    "    keypoints_data = []\n",
    "    for kp, conf in zip(keypoints, confidences):\n",
    "        x, y = kp\n",
    "        visibility = 2 if conf > 0.90 else 0  # Use 2 for visible keypoints, 0 for not visible\n",
    "        if visibility == 0:\n",
    "            x, y = 0, 0\n",
    "        keypoints_data.extend([x, y, visibility])\n",
    "    \n",
    "    return bbox, keypoints_data, results[0]\n",
    "\n",
    "def format_output(bbox, keypoints_data):\n",
    "    # Combine all parts into the required format\n",
    "    output = [0]  # class-index for pitch\n",
    "    output.extend(bbox)  # Bounding box (x_center, y_center, width, height)\n",
    "    output.extend(keypoints_data)  # All keypoints data (x, y, visibility for each keypoint)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "output_image_path = 'dataset_maker/frame_250.jpg'\n",
    "bounding_box, predicted_keypoints, results = predict_keypoints(output_image_path, model)\n",
    "\n",
    "# Format the output\n",
    "formatted_output = format_output(bounding_box, predicted_keypoints)\n",
    "\n",
    "# Print the formatted output for verification\n",
    "print(\"Formatted Output:\", formatted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_on_image(results):\n",
    "    # Use the built-in plot method to draw the results on the image\n",
    "    annotated_image = results.plot(\n",
    "        conf=True,         # Include detection confidence scores\n",
    "        line_width=None,   # Line width of bounding boxes\n",
    "        font_size=None,    # Text font size\n",
    "        font='Arial.ttf',  # Font name for text annotations\n",
    "        pil=False,         # Return image as a PIL Image object\n",
    "        img=None,          # Alternative image for plotting\n",
    "        im_gpu=None,       # GPU-accelerated image for faster mask plotting\n",
    "        kpt_radius=5,      # Radius for drawn keypoints\n",
    "        kpt_line=True,     # Connect keypoints with lines\n",
    "        labels=True,       # Include class labels in annotations\n",
    "        boxes=True,        # Overlay bounding boxes on the image\n",
    "        masks=False,       # Overlay masks on the image\n",
    "        probs=True,        # Include classification probabilities\n",
    "        show=False,        # Display the annotated image directly using the default image viewer\n",
    "        save=False,        # Save the annotated image to a file\n",
    "        filename=None      # Path and name of the file to save the annotated image if save is True\n",
    "    )\n",
    "    \n",
    "    results.show()\n",
    "\n",
    "\n",
    "\n",
    "draw_on_image(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_formatted_output(image_path, formatted_output):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Error: Failed to load image at {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Extract bounding box and keypoints from the formatted output\n",
    "    class_id = formatted_output[0]\n",
    "    bbox = formatted_output[1:5]\n",
    "    keypoints = formatted_output[5:]\n",
    "    \n",
    "    # Draw the bounding box\n",
    "    x_center, y_center, bbox_width, bbox_height = bbox\n",
    "    h, w, _ = image.shape\n",
    "    x_center, y_center = int(x_center * w), int(y_center * h)\n",
    "    bbox_width, bbox_height = int(bbox_width * w), int(bbox_height * h)\n",
    "    x1, y1 = x_center - bbox_width // 2, y_center - bbox_height // 2\n",
    "    x2, y2 = x_center + bbox_width // 2, y_center + bbox_height // 2\n",
    "    \n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # Draw the keypoints\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        if i + 2 >= len(keypoints):\n",
    "            break  # Avoid index out of range error\n",
    "        x, y, vis = keypoints[i], keypoints[i+1], keypoints[i+2]\n",
    "        x, y = int(x * w), int(y * h)\n",
    "        if vis == 2:\n",
    "            cv2.circle(image, (x, y), 5, (0, 0, 255), -1)\n",
    "    \n",
    "    # Display the image\n",
    "    cv2.imshow('Annotated Image', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "output_image_path = 'dataset_maker/frame_250.jpg'\n",
    "\n",
    "\n",
    "# Draw the formatted output on the image\n",
    "draw_formatted_output(output_image_path, formatted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model_path = 'models/Field_Key_Points.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = YOLO(model_path).to(device)\n",
    "\n",
    "def predict_keypoints(image_path, model):\n",
    "    # Predict keypoints using the YOLO model\n",
    "    results = model.predict(image_path, device=device, conf=0.5)\n",
    "    \n",
    "    # Extract results from the prediction\n",
    "    result = results[0]\n",
    "    \n",
    "    # Extract bounding box (normalized)\n",
    "    if result.boxes:\n",
    "        bbox = result.boxes.xyxy[0].tolist()  # xyxy format and convert to list of floats\n",
    "    else:\n",
    "        bbox = [0, 0, 0, 0]\n",
    "    \n",
    "    # Extract keypoints and confidences\n",
    "    keypoints = result.keypoints.xy[0].tolist()  # Keypoints in pixel coordinates\n",
    "    confidences = result.keypoints.conf[0].tolist()  # Confidence scores\n",
    "    \n",
    "    keypoints_data = []\n",
    "    for kp, conf in zip(keypoints, confidences):\n",
    "        x, y = kp\n",
    "        visibility = 2 if conf > 0.90 else 0  # Use 2 for visible keypoints, 0 for not visible\n",
    "        if visibility == 0:\n",
    "            x, y = 0, 0\n",
    "        keypoints_data.append([x, y, visibility])\n",
    "    \n",
    "    return bbox, keypoints_data, result\n",
    "\n",
    "def format_output(bbox, keypoints_data, image_shape):\n",
    "    height, width = image_shape\n",
    "    normalized_keypoints = []\n",
    "    for x, y, visibility in keypoints_data:\n",
    "        x_norm = x / width\n",
    "        y_norm = y / height\n",
    "        normalized_keypoints.extend([x_norm, y_norm, visibility])\n",
    "    \n",
    "    # Combine all parts into the required format\n",
    "    output = [0]  # class-index for pitch\n",
    "    x_center = (bbox[0] + bbox[2]) / 2 / width\n",
    "    y_center = (bbox[1] + bbox[3]) / 2 / height\n",
    "    width_norm = (bbox[2] - bbox[0]) / width\n",
    "    height_norm = (bbox[3] - bbox[1]) / height\n",
    "    output.extend([x_center, y_center, width_norm, height_norm])  # Bounding box (x_center, y_center, width, height)\n",
    "    output.extend(normalized_keypoints)  # All keypoints data (x, y, visibility for each keypoint)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def adjust_keypoints(image_path, bbox, keypoints_data):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Copy the image for drawing\n",
    "    image_copy = image.copy()\n",
    "    \n",
    "    selected_keypoint = None\n",
    "    selected_bbox_corner = None\n",
    "\n",
    "    def draw_keypoints(img, keypoints, bbox):\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "        # Draw keypoints\n",
    "        for i, (x, y, vis) in enumerate(keypoints):\n",
    "            if vis == 2:\n",
    "                cv2.circle(img, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "                cv2.putText(img, str(i+1), (int(x) + 5, int(y) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1, cv2.LINE_AA)\n",
    "        return img\n",
    "\n",
    "    def click_event(event, x, y, flags, param):\n",
    "        nonlocal selected_keypoint, selected_bbox_corner, keypoints_data, image_copy, bbox\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            min_dist = float('inf')\n",
    "            min_index = -1\n",
    "            for i, (kp_x, kp_y, vis) in enumerate(keypoints_data):\n",
    "                if vis == 2:\n",
    "                    dist = np.sqrt((kp_x - x) ** 2 + (kp_y - y) ** 2)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        min_index = i\n",
    "            if min_dist < 10:\n",
    "                selected_keypoint = min_index\n",
    "            else:\n",
    "                selected_keypoint = None\n",
    "                # Check if near bounding box corners\n",
    "                corners = [(bbox[0], bbox[1]), (bbox[2], bbox[1]), (bbox[2], bbox[3]), (bbox[0], bbox[3])]\n",
    "                for i, (cx, cy) in enumerate(corners):\n",
    "                    if np.sqrt((cx - x) ** 2 + (cy - y) ** 2) < 10:\n",
    "                        selected_bbox_corner = i\n",
    "                        break\n",
    "\n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if selected_keypoint is not None:\n",
    "                keypoints_data[selected_keypoint][0] = x\n",
    "                keypoints_data[selected_keypoint][1] = y\n",
    "            elif selected_bbox_corner is not None:\n",
    "                if selected_bbox_corner == 0:\n",
    "                    bbox[0], bbox[1] = x, y\n",
    "                elif selected_bbox_corner == 1:\n",
    "                    bbox[2], bbox[1] = x, y\n",
    "                elif selected_bbox_corner == 2:\n",
    "                    bbox[2], bbox[3] = x, y\n",
    "                elif selected_bbox_corner == 3:\n",
    "                    bbox[0], bbox[3] = x, y\n",
    "            image_copy = image.copy()\n",
    "            draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "            cv2.imshow('Adjust Keypoints', image_copy)\n",
    "\n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            selected_keypoint = None\n",
    "            selected_bbox_corner = None\n",
    "\n",
    "    # Draw initial keypoints and bounding box\n",
    "    image_copy = draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "    cv2.imshow('Adjust Keypoints', image_copy)\n",
    "    cv2.setMouseCallback('Adjust Keypoints', click_event)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return bbox, keypoints_data\n",
    "\n",
    "# Example usage\n",
    "output_image_path = 'dataset_maker/frame_250.jpg'\n",
    "bounding_box, predicted_keypoints, result = predict_keypoints(output_image_path, model)\n",
    "\n",
    "# Adjust keypoints interactively\n",
    "adjusted_bbox, adjusted_keypoints = adjust_keypoints(output_image_path, bounding_box, predicted_keypoints)\n",
    "\n",
    "# Format the output\n",
    "formatted_output = format_output(adjusted_bbox, adjusted_keypoints, result.orig_shape)\n",
    "\n",
    "# Print the formatted output for verification\n",
    "print(\"Formatted Output:\", formatted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model_path = 'models/Field_Key_Points.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = YOLO(model_path).to(device)\n",
    "\n",
    "labels = [\n",
    "    \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\",\n",
    "    \"11\", \"12\", \"13\", \"15\", \"16\", \"17\", \"18\", \"20\", \"21\", \"22\",\n",
    "    \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\",\n",
    "    \"14\", \"19\"\n",
    "]\n",
    "\n",
    "\n",
    "def extract_frame(video_path, frame_number, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(output_path, frame)\n",
    "    cap.release()\n",
    "    return output_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_keypoints(image_path, model):\n",
    "    # Predict keypoints using the YOLO model\n",
    "    results = model.predict(image_path, device=device, conf=0.5)\n",
    "    \n",
    "    # Extract results from the prediction\n",
    "    result = results[0]\n",
    "    \n",
    "    # Extract bounding box (normalized)\n",
    "    if result.boxes:\n",
    "        bbox = result.boxes.xyxy[0].tolist()  # xyxy format and convert to list of floats\n",
    "    else:\n",
    "        bbox = [0, 0, 0, 0]\n",
    "    \n",
    "    # Extract keypoints and confidences\n",
    "    keypoints = result.keypoints.xy[0].tolist()  # Keypoints in pixel coordinates\n",
    "    confidences = result.keypoints.conf[0].tolist()  # Confidence scores\n",
    "    \n",
    "    keypoints_data = []\n",
    "    for kp, conf in zip(keypoints, confidences):\n",
    "        x, y = kp\n",
    "        visibility = 2 if conf > 0.05 else 0  # Use 2 for visible keypoints, 0 for not visible\n",
    "        if visibility == 0:\n",
    "            x, y = 0, 0\n",
    "        keypoints_data.append([x, y, visibility])\n",
    "    \n",
    "    return bbox, keypoints_data, result\n",
    "\n",
    "def format_output(bbox, keypoints_data, image_shape):\n",
    "    height, width = image_shape\n",
    "    normalized_keypoints = []\n",
    "    for x, y, visibility in keypoints_data:\n",
    "        x_norm = x / width\n",
    "        y_norm = y / height\n",
    "        normalized_keypoints.extend([x_norm, y_norm, visibility])\n",
    "    \n",
    "    # Combine all parts into the required format\n",
    "    output = [0]  # class-index for pitch\n",
    "    x_center = (bbox[0] + bbox[2]) / 2 / width\n",
    "    y_center = (bbox[1] + bbox[3]) / 2 / height\n",
    "    width_norm = (bbox[2] - bbox[0]) / width\n",
    "    height_norm = (bbox[3] - bbox[1]) / height\n",
    "    output.extend([x_center, y_center, width_norm, height_norm])  # Bounding box (x_center, y_center, width, height)\n",
    "    output.extend(normalized_keypoints)  # All keypoints data (x, y, visibility for each keypoint)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def adjust_keypoints(image_path, bbox, keypoints_data):\n",
    "    cv2.destroyAllWindows()\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width = image.shape[:2]\n",
    "    \n",
    "    # Copy the image for drawing\n",
    "    image_copy = image.copy()\n",
    "    \n",
    "    selected_keypoint = None\n",
    "    selected_bbox_corner = None\n",
    "    \n",
    "    def draw_keypoints(img, keypoints, bbox):\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "        # Draw keypoints with labels\n",
    "        for i, (x, y, vis) in enumerate(keypoints):\n",
    "            if vis == 2:\n",
    "                cv2.circle(img, (int(x), int(y)), 5, (0, 0, 255), -1)  # Red color for visibility\n",
    "                if i < len(labels):  # Check if the label exists\n",
    "                    cv2.putText(img, labels[i], (int(x) + 7, int(y) - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        return img\n",
    "\n",
    "    def click_event(event, x, y, flags, param):\n",
    "        nonlocal selected_keypoint, selected_bbox_corner, keypoints_data, image_copy, bbox\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            min_dist = float('inf')\n",
    "            min_index = -1\n",
    "            for i, (kp_x, kp_y, vis) in enumerate(keypoints_data):\n",
    "                if vis == 2:\n",
    "                    dist = np.sqrt((kp_x - x) ** 2 + (kp_y - y) ** 2)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        min_index = i\n",
    "            if min_dist < 10:\n",
    "                selected_keypoint = min_index\n",
    "            else:\n",
    "                selected_keypoint = None\n",
    "                # Check if near bounding box corners\n",
    "                corners = [(bbox[0], bbox[1]), (bbox[2], bbox[1]), (bbox[2], bbox[3]), (bbox[0], bbox[3])]\n",
    "                for i, (cx, cy) in enumerate(corners):\n",
    "                    if np.sqrt((cx - x) ** 2 + (cy - y) ** 2) < 10:\n",
    "                        selected_bbox_corner = i\n",
    "                        break\n",
    "\n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if selected_keypoint is not None:\n",
    "                keypoints_data[selected_keypoint][0] = x\n",
    "                keypoints_data[selected_keypoint][1] = y\n",
    "            elif selected_bbox_corner is not None:\n",
    "                if selected_bbox_corner == 0:\n",
    "                    bbox[0], bbox[1] = x, y\n",
    "                elif selected_bbox_corner == 1:\n",
    "                    bbox[2], bbox[1] = x, y\n",
    "                elif selected_bbox_corner == 2:\n",
    "                    bbox[2], bbox[3] = x, y\n",
    "                elif selected_bbox_corner == 3:\n",
    "                    bbox[0], bbox[3] = x, y\n",
    "            image_copy = image.copy()\n",
    "            draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "            cv2.imshow('Adjust Keypoints', image_copy)\n",
    "\n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            selected_keypoint = None\n",
    "            selected_bbox_corner = None\n",
    "\n",
    "    # Draw initial keypoints and bounding box\n",
    "    image_copy = draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "    cv2.imshow('Adjust Keypoints', image_copy)\n",
    "    cv2.setMouseCallback('Adjust Keypoints', click_event)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    return bbox, keypoints_data\n",
    "\n",
    "def save_output(image_path, formatted_output):\n",
    "    # Get the base name of the image file without the extension\n",
    "    base_name = os.path.basename(image_path)\n",
    "    file_name, _ = os.path.splitext(base_name)\n",
    "    \n",
    "    # Define the output file path for the txt file\n",
    "    output_txt_path = os.path.join('dataset_maker', f\"{file_name}.txt\")\n",
    "    \n",
    "    # Write the formatted output to the txt file\n",
    "    with open(output_txt_path, 'w') as f:\n",
    "        f.write(' '.join(map(str, formatted_output)))\n",
    "\n",
    "    print(f\"Saved formatted output to {output_txt_path}\")\n",
    "\n",
    "def draw_on_image(results, filename='dataset_maker/annotated_image.jpg'):\n",
    "    # Plot results image\n",
    "    annotated_image = results.plot(conf=True, kpt_line=True)\n",
    "    \n",
    "    # Show results to screen (in supported environments)\n",
    "    results.show()\n",
    "    \n",
    "    # Save results to disk\n",
    "    results.save(filename)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# Example usage\n",
    "video_path = 'input_videos/antwerp_angle.mp4'\n",
    "frame_number = 250\n",
    "output_image_path = f'dataset_maker/frame_{frame_number}.jpg'\n",
    "extract_frame(video_path, frame_number, output_image_path)\n",
    "\n",
    "\n",
    "bounding_box, predicted_keypoints, result = predict_keypoints(output_image_path, model)\n",
    "\n",
    "# Adjust keypoints interactively\n",
    "adjusted_bbox, adjusted_keypoints = adjust_keypoints(output_image_path, bounding_box, predicted_keypoints)\n",
    "\n",
    "# Format the output\n",
    "formatted_output = format_output(adjusted_bbox, adjusted_keypoints, result.orig_shape)\n",
    "\n",
    "# Print the formatted output for verification\n",
    "print(\"Formatted Output:\", formatted_output)\n",
    "\n",
    "# Save the formatted output to a .txt file\n",
    "save_output(output_image_path, formatted_output)\n",
    "\n",
    "draw_on_image(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\dataset_maker\\frame_250.jpg: 384x640 1 pitch, 31.0ms\n",
      "Speed: 3.0ms preprocess, 31.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 1\n",
      "Current label: 18\n",
      "Labeled point 1 as 18\n",
      "Formatted Output: [0, 0.4986979166666667, 0.5, 0.9963541666666667, 1.0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.0, 0.0, 0, 0.096875, 0.07777777777777778, 2, 0.10885416666666667, 0.24166666666666667, 2, 0.12291666666666666, 0.43425925925925923, 2, 0.1515625, 0.9175925925925926, 2, 0.6046875, 0.09907407407407408, 2, 0.6713541666666667, 0.17777777777777778, 2, 0.7703125, 0.2953703703703704, 2, 0.8958333333333334, 0.4398148148148148, 2, 0.796875, 0.21851851851851853, 2, 0.809375, 0.14166666666666666, 2, 0.9661458333333334, 0.2796296296296296, 2, 0.725, 0.009259259259259259, 2, 0.7994791666666666, 0.06759259259259259, 2, 0.875, 0.1287037037037037, 2, 0.0, 0.0, 2, 0.0, 0.0, 2, 0.0, 0.0, 0, 0.0, 0.0, 2, 0.28125, 0.30462962962962964, 2]\n",
      "Saved formatted output to dataset_maker\\frame_250.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model_path = 'models/Field_Key_Points.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = YOLO(model_path).to(device)\n",
    "\n",
    "labels = [\n",
    "    \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\",\n",
    "    \"11\", \"12\", \"13\", \"15\", \"16\", \"17\", \"18\", \"20\", \"21\", \"22\",\n",
    "    \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\",\n",
    "    \"14\", \"19\"\n",
    "]\n",
    "\n",
    "def extract_frame(video_path, frame_number, output_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(output_path, frame)\n",
    "    cap.release()\n",
    "    return output_path\n",
    "\n",
    "def predict_keypoints(image_path, model):\n",
    "    results = model.predict(image_path, device=device, conf=0.5)\n",
    "    result = results[0]\n",
    "    if result.boxes:\n",
    "        bbox = result.boxes.xyxy[0].tolist()\n",
    "    else:\n",
    "        bbox = [0, 0, 0, 0]\n",
    "    keypoints = result.keypoints.xy[0].tolist()\n",
    "    confidences = result.keypoints.conf[0].tolist()\n",
    "    keypoints_data = []\n",
    "    for kp, conf in zip(keypoints, confidences):\n",
    "        x, y = kp\n",
    "        visibility = 2 if conf > 0.05 else 0\n",
    "        if visibility == 0:\n",
    "            x, y = 0, 0\n",
    "        keypoints_data.append([x, y, visibility])\n",
    "    return bbox, keypoints_data, result\n",
    "\n",
    "def format_output(bbox, keypoints_data, image_shape):\n",
    "    height, width = image_shape\n",
    "    normalized_keypoints = []\n",
    "    for x, y, visibility in keypoints_data:\n",
    "        x_norm = x / width\n",
    "        y_norm = y / height\n",
    "        normalized_keypoints.extend([x_norm, y_norm, visibility])\n",
    "    output = [0]\n",
    "    x_center = (bbox[0] + bbox[2]) / 2 / width\n",
    "    y_center = (bbox[1] + bbox[3]) / 2 / height\n",
    "    width_norm = (bbox[2] - bbox[0]) / width\n",
    "    height_norm = (bbox[3] - bbox[1]) / height\n",
    "    output.extend([x_center, y_center, width_norm, height_norm])\n",
    "    output.extend(normalized_keypoints)\n",
    "    return output\n",
    "\n",
    "def adjust_keypoints(image_path, bbox, keypoints_data):\n",
    "    cv2.destroyAllWindows()\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width = image.shape[:2]\n",
    "    image_copy = image.copy()\n",
    "    selected_keypoint = None\n",
    "    selected_bbox_corner = None\n",
    "    new_points = []\n",
    "    point_to_label = {}\n",
    "\n",
    "    def draw_keypoints(img, keypoints, bbox):\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "        for i, (x, y, vis) in enumerate(keypoints):\n",
    "            if vis == 2:\n",
    "                cv2.circle(img, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "                if i < len(labels):\n",
    "                    cv2.putText(img, labels[i], (int(x) + 7, int(y) - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        for (x, y, label) in new_points:\n",
    "            cv2.circle(img, (int(x), int(y)), 5, (255, 0, 0), -1)\n",
    "            cv2.putText(img, label, (int(x) + 7, int(y) - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        return img\n",
    "\n",
    "    def click_event(event, x, y, flags, param):\n",
    "        nonlocal selected_keypoint, selected_bbox_corner, keypoints_data, image_copy, bbox, new_points\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            min_dist = float('inf')\n",
    "            min_index = -1\n",
    "            for i, (kp_x, kp_y, vis) in enumerate(keypoints_data):\n",
    "                if vis == 2:\n",
    "                    dist = np.sqrt((kp_x - x) ** 2 + (kp_y - y) ** 2)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        min_index = i\n",
    "            for i, (kp_x, kp_y, _) in enumerate(new_points):\n",
    "                dist = np.sqrt((kp_x - x) ** 2 + (kp_y - y) ** 2)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_index = i + len(keypoints_data)\n",
    "            if min_dist < 10:\n",
    "                selected_keypoint = min_index\n",
    "            else:\n",
    "                selected_keypoint = None\n",
    "                corners = [(bbox[0], bbox[1]), (bbox[2], bbox[1]), (bbox[2], bbox[3]), (bbox[0], bbox[3])]\n",
    "                for i, (cx, cy) in enumerate(corners):\n",
    "                    if np.sqrt((cx - x) ** 2 + (cy - y) ** 2) < 10:\n",
    "                        selected_bbox_corner = i\n",
    "                        break\n",
    "        elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "            new_points.append((x, y, \"\"))\n",
    "            draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "            cv2.imshow('Adjust Keypoints', image_copy)\n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if selected_keypoint is not None:\n",
    "                if selected_keypoint < len(keypoints_data):\n",
    "                    keypoints_data[selected_keypoint][0] = x\n",
    "                    keypoints_data[selected_keypoint][1] = y\n",
    "                else:\n",
    "                    new_points[selected_keypoint - len(keypoints_data)] = (x, y, new_points[selected_keypoint - len(keypoints_data)][2])\n",
    "            elif selected_bbox_corner is not None:\n",
    "                if selected_bbox_corner == 0:\n",
    "                    bbox[0], bbox[1] = x, y\n",
    "                elif selected_bbox_corner == 1:\n",
    "                    bbox[2], bbox[1] = x, y\n",
    "                elif selected_bbox_corner == 2:\n",
    "                    bbox[2], bbox[3] = x, y\n",
    "                elif selected_bbox_corner == 3:\n",
    "                    bbox[0], bbox[3] = x, y\n",
    "            image_copy = image.copy()\n",
    "            draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "            cv2.imshow('Adjust Keypoints', image_copy)\n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            selected_keypoint = None\n",
    "            selected_bbox_corner = None\n",
    "\n",
    "    image_copy = draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "    cv2.imshow('Adjust Keypoints', image_copy)\n",
    "    cv2.setMouseCallback('Adjust Keypoints', click_event)\n",
    "\n",
    "    current_label = \"\"\n",
    "    while True:\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27:  # ESC to exit\n",
    "            break\n",
    "        elif key == 13:  # Enter to confirm label\n",
    "            if new_points and current_label:\n",
    "                new_points[-1] = (new_points[-1][0], new_points[-1][1], current_label)\n",
    "                print(f\"Labeled point {len(new_points)} as {current_label}\")\n",
    "                current_label = \"\"\n",
    "                image_copy = image.copy()\n",
    "                draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "                cv2.imshow('Adjust Keypoints', image_copy)\n",
    "        elif ord('0') <= key <= ord('9'):\n",
    "            current_label += chr(key)\n",
    "            print(f\"Current label: {current_label}\")\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    for (x, y, label) in new_points:\n",
    "        if label:\n",
    "            index = labels.index(label)\n",
    "            keypoints_data[index] = [x, y, 2]\n",
    "\n",
    "    return bbox, keypoints_data\n",
    "\n",
    "def save_output(image_path, formatted_output):\n",
    "    base_name = os.path.basename(image_path)\n",
    "    file_name, _ = os.path.splitext(base_name)\n",
    "    output_txt_path = os.path.join('dataset_maker', f\"{file_name}.txt\")\n",
    "    with open(output_txt_path, 'w') as f:\n",
    "        f.write(' '.join(map(str, formatted_output)))\n",
    "    print(f\"Saved formatted output to {output_txt_path}\")\n",
    "\n",
    "def draw_on_image(results, filename='dataset_maker/annotated_image.jpg'):\n",
    "    annotated_image = results.plot(conf=True, kpt_line=True)\n",
    "    results.show()\n",
    "    results.save(filename)\n",
    "\n",
    "video_path = 'input_videos/antwerp_angle.mp4'\n",
    "frame_number = 250\n",
    "output_image_path = f'dataset_maker/frame_{frame_number}.jpg'\n",
    "extract_frame(video_path, frame_number, output_image_path)\n",
    "bounding_box, predicted_keypoints, result = predict_keypoints(output_image_path, model)\n",
    "adjusted_bbox, adjusted_keypoints = adjust_keypoints(output_image_path, bounding_box, predicted_keypoints)\n",
    "formatted_output = format_output(adjusted_bbox, adjusted_keypoints, result.orig_shape)\n",
    "print(\"Formatted Output:\", formatted_output)\n",
    "save_output(output_image_path, formatted_output)\n",
    "draw_on_image(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 1/50\n",
      "\n",
      "0: 384x640 1 pitch, 808.0ms\n",
      "Speed: 3.0ms preprocess, 808.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 1\n",
      "Current label: 11\n",
      "Labeled point 1 as 11\n",
      "Current label: 0\n",
      "Current label: 02\n",
      "Labeled point 2 as 02\n",
      "Current label: 0\n",
      "Current label: 01\n",
      "Labeled point 3 as 01\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_0.txt\n",
      "Frame 2/50\n",
      "\n",
      "0: 384x640 1 pitch, 776.2ms\n",
      "Speed: 9.0ms preprocess, 776.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 0\n",
      "Current label: 02\n",
      "Labeled point 1 as 02\n",
      "Current label: 0\n",
      "Current label: 01\n",
      "Labeled point 2 as 01\n",
      "Current label: 1\n",
      "Current label: 10\n",
      "Labeled point 3 as 10\n",
      "Current label: 1\n",
      "Current label: 11\n",
      "Labeled point 4 as 11\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_15.txt\n",
      "Frame 3/50\n",
      "\n",
      "0: 384x640 1 pitch, 770.0ms\n",
      "Speed: 44.0ms preprocess, 770.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 0\n",
      "Current label: 01\n",
      "Labeled point 1 as 01\n",
      "Current label: 1\n",
      "Current label: 10\n",
      "Labeled point 2 as 10\n",
      "Current label: 1\n",
      "Current label: 11\n",
      "Labeled point 3 as 11\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_30.txt\n",
      "Frame 4/50\n",
      "\n",
      "0: 384x640 1 pitch, 641.0ms\n",
      "Speed: 3.0ms preprocess, 641.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 0\n",
      "Current label: 01\n",
      "Labeled point 1 as 01\n",
      "Current label: 1\n",
      "Current label: 11\n",
      "Labeled point 2 as 11\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_45.txt\n",
      "Frame 5/50\n",
      "\n",
      "0: 384x640 1 pitch, 756.0ms\n",
      "Speed: 43.0ms preprocess, 756.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 0\n",
      "Current label: 01\n",
      "Labeled point 1 as 01\n",
      "Current label: 0\n",
      "Current label: 02\n",
      "Labeled point 2 as 02\n",
      "Current label: 1\n",
      "Current label: 10\n",
      "Labeled point 3 as 10\n",
      "Current label: 1\n",
      "Current label: 11\n",
      "Labeled point 4 as 11\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_60.txt\n",
      "Frame 6/50\n",
      "\n",
      "0: 384x640 1 pitch, 649.0ms\n",
      "Speed: 5.0ms preprocess, 649.0ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_75.txt\n",
      "Frame 7/50\n",
      "\n",
      "0: 384x640 1 pitch, 793.0ms\n",
      "Speed: 7.0ms preprocess, 793.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_90.txt\n",
      "Frame 8/50\n",
      "\n",
      "0: 384x640 1 pitch, 822.0ms\n",
      "Speed: 6.0ms preprocess, 822.0ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_105.txt\n",
      "Frame 9/50\n",
      "\n",
      "0: 384x640 1 pitch, 639.0ms\n",
      "Speed: 11.0ms preprocess, 639.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 0\n",
      "Current label: 08\n",
      "Labeled point 1 as 08\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_120.txt\n",
      "Frame 10/50\n",
      "\n",
      "0: 384x640 1 pitch, 711.5ms\n",
      "Speed: 7.0ms preprocess, 711.5ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 1\n",
      "Current label: 10\n",
      "Labeled point 1 as 10\n",
      "Current label: 1\n",
      "Current label: 13\n",
      "Labeled point 2 as 13\n",
      "Current label: 0\n",
      "Current label: 04\n",
      "Labeled point 3 as 04\n",
      "Current label: 0\n",
      "Current label: 08\n",
      "Labeled point 4 as 08\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_135.txt\n",
      "Frame 11/50\n",
      "\n",
      "0: 384x640 1 pitch, 758.9ms\n",
      "Speed: 6.0ms preprocess, 758.9ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 1\n",
      "Current label: 10\n",
      "Labeled point 1 as 10\n",
      "Current label: 1\n",
      "Current label: 13\n",
      "Labeled point 2 as 13\n",
      "Current label: 0\n",
      "Current label: 04\n",
      "Labeled point 3 as 04\n",
      "Current label: 0\n",
      "Current label: 08\n",
      "Labeled point 4 as 08\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_150.txt\n",
      "Frame 12/50\n",
      "\n",
      "0: 384x640 1 pitch, 906.2ms\n",
      "Speed: 6.0ms preprocess, 906.2ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 1\n",
      "Current label: 10\n",
      "Labeled point 1 as 10\n",
      "Current label: 1\n",
      "Current label: 13\n",
      "Labeled point 2 as 13\n",
      "Current label: 0\n",
      "Current label: 08\n",
      "Labeled point 3 as 08\n",
      "Current label: 0\n",
      "Current label: 04\n",
      "Labeled point 4 as 04\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_165.txt\n",
      "Frame 13/50\n",
      "\n",
      "0: 384x640 1 pitch, 735.3ms\n",
      "Speed: 5.0ms preprocess, 735.3ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 0\n",
      "Current label: 07\n",
      "Labeled point 1 as 07\n",
      "Current label: 1\n",
      "Current label: 10\n",
      "Labeled point 1 as 10\n",
      "Current label: 0\n",
      "Current label: 08\n",
      "Labeled point 2 as 08\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_180.txt\n",
      "Frame 14/50\n",
      "\n",
      "0: 384x640 1 pitch, 872.0ms\n",
      "Speed: 16.3ms preprocess, 872.0ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_195.txt\n",
      "Frame 15/50\n",
      "\n",
      "0: 384x640 1 pitch, 913.5ms\n",
      "Speed: 5.0ms preprocess, 913.5ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_210.txt\n",
      "Frame 16/50\n",
      "\n",
      "0: 384x640 1 pitch, 746.8ms\n",
      "Speed: 6.0ms preprocess, 746.8ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_225.txt\n",
      "Frame 17/50\n",
      "\n",
      "0: 384x640 1 pitch, 775.6ms\n",
      "Speed: 215.6ms preprocess, 775.6ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_240.txt\n",
      "Frame 18/50\n",
      "\n",
      "0: 384x640 1 pitch, 721.3ms\n",
      "Speed: 27.0ms preprocess, 721.3ms inference, 12.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_255.txt\n",
      "Frame 19/50\n",
      "\n",
      "0: 384x640 1 pitch, 800.7ms\n",
      "Speed: 6.0ms preprocess, 800.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_270.txt\n",
      "Frame 20/50\n",
      "\n",
      "0: 384x640 1 pitch, 757.2ms\n",
      "Speed: 9.0ms preprocess, 757.2ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 0\n",
      "Current label: 01\n",
      "Labeled point 1 as 01\n",
      "Current label: 1\n",
      "Current label: 10\n",
      "Labeled point 2 as 10\n",
      "Current label: 1\n",
      "Current label: 11\n",
      "Labeled point 3 as 11\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_285.txt\n",
      "Frame 21/50\n",
      "\n",
      "0: 384x640 1 pitch, 746.1ms\n",
      "Speed: 8.0ms preprocess, 746.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_300.txt\n",
      "Frame 22/50\n",
      "\n",
      "0: 384x640 1 pitch, 903.7ms\n",
      "Speed: 5.0ms preprocess, 903.7ms inference, 13.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_315.txt\n",
      "Frame 23/50\n",
      "\n",
      "0: 384x640 1 pitch, 761.0ms\n",
      "Speed: 41.5ms preprocess, 761.0ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_330.txt\n",
      "Frame 24/50\n",
      "\n",
      "0: 384x640 1 pitch, 746.7ms\n",
      "Speed: 5.0ms preprocess, 746.7ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_345.txt\n",
      "Frame 25/50\n",
      "\n",
      "0: 384x640 1 pitch, 723.7ms\n",
      "Speed: 7.0ms preprocess, 723.7ms inference, 8.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 2 as 21\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_360.txt\n",
      "Frame 26/50\n",
      "\n",
      "0: 384x640 1 pitch, 764.1ms\n",
      "Speed: 6.0ms preprocess, 764.1ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 20\n",
      "Labeled point 1 as 20\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 2 as 27\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 3 as 21\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_375.txt\n",
      "Frame 27/50\n",
      "\n",
      "0: 384x640 1 pitch, 757.6ms\n",
      "Speed: 7.0ms preprocess, 757.6ms inference, 10.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 1\n",
      "Current label: 19\n",
      "Labeled point 1 as 19\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 2 as 27\n",
      "Current label: 2\n",
      "Current label: 20\n",
      "Labeled point 3 as 20\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 4 as 28\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 5 as 21\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_390.txt\n",
      "Frame 28/50\n",
      "\n",
      "0: 384x640 1 pitch, 718.9ms\n",
      "Speed: 10.0ms preprocess, 718.9ms inference, 12.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 3 as 25\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_405.txt\n",
      "Frame 29/50\n",
      "\n",
      "0: 384x640 1 pitch, 697.7ms\n",
      "Speed: 41.0ms preprocess, 697.7ms inference, 11.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_420.txt\n",
      "Frame 30/50\n",
      "\n",
      "0: 384x640 1 pitch, 357.2ms\n",
      "Speed: 40.0ms preprocess, 357.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_435.txt\n",
      "Frame 31/50\n",
      "\n",
      "0: 384x640 1 pitch, 479.7ms\n",
      "Speed: 5.0ms preprocess, 479.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_450.txt\n",
      "Frame 32/50\n",
      "\n",
      "0: 384x640 1 pitch, 375.3ms\n",
      "Speed: 5.0ms preprocess, 375.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_465.txt\n",
      "Frame 33/50\n",
      "\n",
      "0: 384x640 1 pitch, 175.0ms\n",
      "Speed: 6.0ms preprocess, 175.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_480.txt\n",
      "Frame 34/50\n",
      "\n",
      "0: 384x640 1 pitch, 193.0ms\n",
      "Speed: 9.0ms preprocess, 193.0ms inference, 7.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_495.txt\n",
      "Frame 35/50\n",
      "\n",
      "0: 384x640 1 pitch, 347.2ms\n",
      "Speed: 10.0ms preprocess, 347.2ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 20\n",
      "Labeled point 1 as 20\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 2 as 22\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 2 as 21\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_510.txt\n",
      "Frame 36/50\n",
      "\n",
      "0: 384x640 2 pitchs, 305.0ms\n",
      "Speed: 98.0ms preprocess, 305.0ms inference, 19.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 20\n",
      "Labeled point 1 as 20\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_525.txt\n",
      "Frame 37/50\n",
      "\n",
      "0: 384x640 2 pitchs, 388.2ms\n",
      "Speed: 12.0ms preprocess, 388.2ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 20\n",
      "Labeled point 1 as 20\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_540.txt\n",
      "Frame 38/50\n",
      "\n",
      "0: 384x640 2 pitchs, 356.4ms\n",
      "Speed: 7.0ms preprocess, 356.4ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 20\n",
      "Labeled point 2 as 20\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 3 as 21\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 4 as 22\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 5 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 6 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 7 as 23\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_555.txt\n",
      "Frame 39/50\n",
      "\n",
      "0: 384x640 1 pitch, 297.1ms\n",
      "Speed: 4.0ms preprocess, 297.1ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 20\n",
      "Labeled point 3 as 20\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 4 as 21\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 5 as 22\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 6 as 25\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 7 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 8 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 9 as 23\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_570.txt\n",
      "Frame 40/50\n",
      "\n",
      "0: 384x640 1 pitch, 287.6ms\n",
      "Speed: 6.0ms preprocess, 287.6ms inference, 9.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 3 as 25\n",
      "Current label: 2\n",
      "Current label: 29\n",
      "Labeled point 4 as 29\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 5 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 6 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 7 as 23\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 8 as 22\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 9 as 21\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_585.txt\n",
      "Frame 41/50\n",
      "\n",
      "0: 384x640 2 pitchs, 311.0ms\n",
      "Speed: 9.0ms preprocess, 311.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 3 as 25\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 4 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 5 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 6 as 23\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 7 as 22\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 8 as 21\n",
      "Current label: 2\n",
      "Current label: 24\n",
      "Labeled point 9 as 24\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_600.txt\n",
      "Frame 42/50\n",
      "\n",
      "0: 384x640 1 pitch, 328.0ms\n",
      "Speed: 14.0ms preprocess, 328.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 29\n",
      "Labeled point 3 as 29\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 4 as 25\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 5 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 6 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 7 as 23\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 8 as 22\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 9 as 21\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_615.txt\n",
      "Frame 43/50\n",
      "\n",
      "0: 384x640 1 pitch, 379.0ms\n",
      "Speed: 5.0ms preprocess, 379.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 2 as 25\n",
      "Current label: 2\n",
      "Current label: 29\n",
      "Labeled point 3 as 29\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 4 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 5 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 6 as 23\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 7 as 22\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 8 as 21\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 9 as 28\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_630.txt\n",
      "Frame 44/50\n",
      "\n",
      "0: 384x640 1 pitch, 138.0ms\n",
      "Speed: 8.0ms preprocess, 138.0ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 29\n",
      "Labeled point 3 as 29\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 4 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 5 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 6 as 23\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 7 as 22\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 8 as 21\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_645.txt\n",
      "Frame 45/50\n",
      "\n",
      "0: 384x640 1 pitch, 308.0ms\n",
      "Speed: 5.0ms preprocess, 308.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 3 as 25\n",
      "Current label: 2\n",
      "Current label: 29\n",
      "Labeled point 4 as 29\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 5 as 30\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 6 as 26\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 7 as 23\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 8 as 22\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 9 as 21\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_660.txt\n",
      "Frame 46/50\n",
      "\n",
      "0: 384x640 1 pitch, 346.1ms\n",
      "Speed: 6.0ms preprocess, 346.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 3 as 25\n",
      "Current label: 2\n",
      "Current label: 29\n",
      "Labeled point 4 as 29\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 5 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 6 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 7 as 23\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 8 as 22\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 9 as 25\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 9 as 21\n",
      "Current label: 2\n",
      "Current label: 24\n",
      "Labeled point 10 as 24\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_675.txt\n",
      "Frame 47/50\n",
      "\n",
      "0: 384x640 1 pitch, 153.0ms\n",
      "Speed: 5.0ms preprocess, 153.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 3 as 25\n",
      "Current label: 2\n",
      "Current label: 29\n",
      "Labeled point 4 as 29\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 5 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 6 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 7 as 23\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 8 as 22\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 9 as 21\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_690.txt\n",
      "Frame 48/50\n",
      "\n",
      "0: 384x640 1 pitch, 336.6ms\n",
      "Speed: 12.0ms preprocess, 336.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 3 as 21\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 4 as 22\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 5 as 23\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 6 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 7 as 30\n",
      "Current label: 2\n",
      "Current label: 29\n",
      "Labeled point 8 as 29\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 9 as 25\n",
      "Current label: 2\n",
      "Current label: 24\n",
      "Labeled point 10 as 24\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_705.txt\n",
      "Frame 49/50\n",
      "\n",
      "0: 384x640 1 pitch, 284.0ms\n",
      "Speed: 5.0ms preprocess, 284.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 3 as 25\n",
      "Current label: 2\n",
      "Current label: 29\n",
      "Labeled point 4 as 29\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 5 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 6 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 7 as 23\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 8 as 22\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 9 as 21\n",
      "Current label: 2\n",
      "Current label: 24\n",
      "Labeled point 10 as 24\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_720.txt\n",
      "Frame 50/50\n",
      "\n",
      "0: 384x640 1 pitch, 304.0ms\n",
      "Speed: 5.0ms preprocess, 304.0ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Current label: 2\n",
      "Current label: 27\n",
      "Labeled point 1 as 27\n",
      "Current label: 2\n",
      "Current label: 28\n",
      "Labeled point 2 as 28\n",
      "Current label: 2\n",
      "Current label: 25\n",
      "Labeled point 3 as 25\n",
      "Current label: 2\n",
      "Current label: 29\n",
      "Labeled point 4 as 29\n",
      "Current label: 2\n",
      "Current label: 26\n",
      "Labeled point 5 as 26\n",
      "Current label: 3\n",
      "Current label: 30\n",
      "Labeled point 6 as 30\n",
      "Current label: 2\n",
      "Current label: 23\n",
      "Labeled point 7 as 23\n",
      "Current label: 2\n",
      "Current label: 22\n",
      "Labeled point 8 as 22\n",
      "Current label: 2\n",
      "Current label: 21\n",
      "Labeled point 9 as 21\n",
      "Saved formatted output to dataset_maker/labels\\riga_frame_735.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLOv8 model\n",
    "model_path = 'models/key_points_2.pt'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = YOLO(model_path).to(device)\n",
    "\n",
    "labels = [\n",
    "    \"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\",\n",
    "    \"11\", \"12\", \"13\", \"15\", \"16\", \"17\", \"18\", \"20\", \"21\", \"22\",\n",
    "    \"23\", \"24\", \"25\", \"26\", \"27\", \"28\", \"29\", \"30\", \"31\", \"32\",\n",
    "    \"14\", \"19\"\n",
    "]\n",
    "\n",
    "def create_directories():\n",
    "    if not os.path.exists('dataset_maker/images'):\n",
    "        os.makedirs('dataset_maker/images')\n",
    "    if not os.path.exists('dataset_maker/labels'):\n",
    "        os.makedirs('dataset_maker/labels')\n",
    "\n",
    "def extract_frame(video_path, frame_number):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    return ret, frame\n",
    "\n",
    "def save_frame(output_path, frame):\n",
    "    cv2.imwrite(output_path, frame)\n",
    "\n",
    "def predict_keypoints(image_path, model):\n",
    "    results = model.predict(image_path, device=device, conf=0.5)\n",
    "    result = results[0]\n",
    "    if result.boxes:\n",
    "        bbox = result.boxes.xyxy[0].tolist()\n",
    "    else:\n",
    "        bbox = [0, 0, 0, 0]\n",
    "    keypoints = result.keypoints.xy[0].tolist() if len(result.keypoints) > 0 else []\n",
    "    confidences = result.keypoints.conf[0].tolist() if result.keypoints.conf is not None and len(result.keypoints.conf) > 0 else []\n",
    "    keypoints_data = []\n",
    "    for kp, conf in zip(keypoints, confidences):\n",
    "        x, y = kp\n",
    "        visibility = 2 if conf > 0.70 else 0\n",
    "        if visibility == 0:\n",
    "            x, y = 0, 0\n",
    "        keypoints_data.append([x, y, visibility])\n",
    "    return bbox, keypoints_data, result\n",
    "\n",
    "def format_output(bbox, keypoints_data, image_shape):\n",
    "    height, width = image_shape\n",
    "    normalized_keypoints = []\n",
    "    for x, y, visibility in keypoints_data:\n",
    "        x_norm = x / width\n",
    "        y_norm = y / height\n",
    "        normalized_keypoints.extend([x_norm, y_norm, visibility])\n",
    "    output = [0]\n",
    "    x_center = (bbox[0] + bbox[2]) / 2 / width\n",
    "    y_center = (bbox[1] + bbox[3]) / 2 / height\n",
    "    width_norm = (bbox[2] - bbox[0]) / width\n",
    "    height_norm = (bbox[3] - bbox[1]) / height\n",
    "    output.extend([x_center, y_center, width_norm, height_norm])\n",
    "    output.extend(normalized_keypoints)\n",
    "    return output\n",
    "\n",
    "def adjust_keypoints(image_path, bbox, keypoints_data, current_frame, total_frames):\n",
    "    cv2.destroyAllWindows()\n",
    "    image = cv2.imread(image_path)\n",
    "    height, width = image.shape[:2]\n",
    "    image_copy = image.copy()\n",
    "    selected_keypoint = None\n",
    "    selected_bbox_corner = None\n",
    "    new_points = []\n",
    "    point_to_label = {}\n",
    "    skip_frame = False\n",
    "    exit_program = False\n",
    "\n",
    "    def draw_keypoints(img, keypoints, bbox):\n",
    "        cv2.rectangle(img, (int(bbox[0]), int(bbox[1])), (int(bbox[2]), int(bbox[3])), (0, 255, 0), 2)\n",
    "        for i, (x, y, vis) in enumerate(keypoints):\n",
    "            if vis == 2:\n",
    "                cv2.circle(img, (int(x), int(y)), 5, (0, 0, 255), -1)\n",
    "                if i < len(labels):\n",
    "                    cv2.putText(img, labels[i], (int(x) + 7, int(y) - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        for (x, y, label) in new_points:\n",
    "            cv2.circle(img, (int(x), int(y)), 5, (255, 0, 0), -1)\n",
    "            cv2.putText(img, label, (int(x) + 7, int(y) - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        cv2.putText(img, f'Frame {current_frame}/{total_frames}', (10, height - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        cv2.putText(img, 'Press ESC to save and go to next frame, S to skip frame, E to exit', (10, height - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "        return img\n",
    "\n",
    "    def click_event(event, x, y, flags, param):\n",
    "        nonlocal selected_keypoint, selected_bbox_corner, keypoints_data, image_copy, bbox, new_points\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            min_dist = float('inf')\n",
    "            min_index = -1\n",
    "            for i, (kp_x, kp_y, vis) in enumerate(keypoints_data):\n",
    "                if vis == 2:\n",
    "                    dist = np.sqrt((kp_x - x) ** 2 + (kp_y - y) ** 2)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist\n",
    "                        min_index = i\n",
    "            for i, (kp_x, kp_y, _) in enumerate(new_points):\n",
    "                dist = np.sqrt((kp_x - x) ** 2 + (kp_y - y) ** 2)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_index = i + len(keypoints_data)\n",
    "            if min_dist < 10:\n",
    "                selected_keypoint = min_index\n",
    "            else:\n",
    "                selected_keypoint = None\n",
    "                corners = [(bbox[0], bbox[1]), (bbox[2], bbox[1]), (bbox[2], bbox[3]), (bbox[0], bbox[3])]\n",
    "                for i, (cx, cy) in enumerate(corners):\n",
    "                    if np.sqrt((cx - x) ** 2 + (cy - y) ** 2) < 10:\n",
    "                        selected_bbox_corner = i\n",
    "                        break\n",
    "        elif event == cv2.EVENT_RBUTTONDOWN:\n",
    "            new_points.append((x, y, \"\"))\n",
    "            draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "            cv2.imshow('Adjust Keypoints', image_copy)\n",
    "        elif event == cv2.EVENT_MOUSEMOVE:\n",
    "            if selected_keypoint is not None:\n",
    "                if selected_keypoint < len(keypoints_data):\n",
    "                    keypoints_data[selected_keypoint][0] = x\n",
    "                    keypoints_data[selected_keypoint][1] = y\n",
    "                else:\n",
    "                    new_points[selected_keypoint - len(keypoints_data)] = (x, y, new_points[selected_keypoint - len(keypoints_data)][2])\n",
    "            elif selected_bbox_corner is not None:\n",
    "                if selected_bbox_corner == 0:\n",
    "                    bbox[0], bbox[1] = x, y\n",
    "                elif selected_bbox_corner == 1:\n",
    "                    bbox[2], bbox[1] = x, y\n",
    "                elif selected_bbox_corner == 2:\n",
    "                    bbox[2], bbox[3] = x, y\n",
    "                elif selected_bbox_corner == 3:\n",
    "                    bbox[0], bbox[3] = x, y\n",
    "            image_copy = image.copy()\n",
    "            draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "            cv2.imshow('Adjust Keypoints', image_copy)\n",
    "        elif event == cv2.EVENT_LBUTTONUP:\n",
    "            selected_keypoint = None\n",
    "            selected_bbox_corner = None\n",
    "\n",
    "    image_copy = draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "    cv2.imshow('Adjust Keypoints', image_copy)\n",
    "    cv2.setMouseCallback('Adjust Keypoints', click_event)\n",
    "\n",
    "    current_label = \"\"\n",
    "    while True:\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == 27:  # ESC to exit and save\n",
    "            break\n",
    "        elif key == 13:  # Enter to confirm label\n",
    "            if new_points and current_label:\n",
    "                new_points[-1] = (new_points[-1][0], new_points[-1][1], current_label)\n",
    "                print(f\"Labeled point {len(new_points)} as {current_label}\")\n",
    "                current_label = \"\"\n",
    "                image_copy = image.copy()\n",
    "                draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "                cv2.imshow('Adjust Keypoints', image_copy)\n",
    "        elif ord('0') <= key <= ord('9'):\n",
    "            current_label += chr(key)\n",
    "            print(f\"Current label: {current_label}\")\n",
    "        elif key == ord('d') and selected_keypoint is not None:\n",
    "            if selected_keypoint < len(keypoints_data):\n",
    "                keypoints_data[selected_keypoint] = [0, 0, 0]\n",
    "            else:\n",
    "                new_points.pop(selected_keypoint - len(keypoints_data))\n",
    "            selected_keypoint = None\n",
    "            image_copy = image.copy()\n",
    "            draw_keypoints(image_copy, keypoints_data, bbox)\n",
    "            cv2.imshow('Adjust Keypoints', image_copy)\n",
    "        elif key == ord('s'):  # Skip frame\n",
    "            skip_frame = True\n",
    "            break\n",
    "        elif key == ord('e'):  # Exit program\n",
    "            exit_program = True\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if skip_frame:\n",
    "        return None, None, False\n",
    "    if exit_program:\n",
    "        return None, None, True\n",
    "\n",
    "    for (x, y, label) in new_points:\n",
    "        if label:\n",
    "            index = labels.index(label)\n",
    "            keypoints_data[index] = [x, y, 2]\n",
    "\n",
    "    return bbox, keypoints_data, False\n",
    "\n",
    "def save_output(image_path, formatted_output, video_name):\n",
    "    base_name = os.path.basename(image_path)\n",
    "    file_name, _ = os.path.splitext(base_name)\n",
    "    output_txt_path = os.path.join('dataset_maker/labels', f\"{file_name}.txt\")\n",
    "    with open(output_txt_path, 'w') as f:\n",
    "        f.write(' '.join(map(str, formatted_output)))\n",
    "    print(f\"Saved formatted output to {output_txt_path}\")\n",
    "\n",
    "def draw_on_image(results, filename='dataset_maker/annotated_image.jpg'):\n",
    "    annotated_image = results.plot(conf=True, kpt_line=True)\n",
    "    results.show()\n",
    "    results.save(filename)\n",
    "\n",
    "def main(video_path):\n",
    "    create_directories()\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frames_to_sample = frame_count // 50\n",
    "    frame_number = 0\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    valid_frame_count = 0\n",
    "\n",
    "    while frame_number < frame_count and valid_frame_count < 50:\n",
    "        print(f\"Frame {valid_frame_count + 1}/50\")\n",
    "        while True:\n",
    "            ret, frame = extract_frame(video_path, frame_number)\n",
    "            if not ret:\n",
    "                frame_number += frames_to_sample\n",
    "                if frame_number >= frame_count:\n",
    "                    cap.release()\n",
    "                    return\n",
    "                continue\n",
    "            \n",
    "            bounding_box, predicted_keypoints, result = predict_keypoints(frame, model)\n",
    "            if predicted_keypoints:\n",
    "                output_image_path = f'dataset_maker/images/{video_name}_frame_{frame_number}.jpg'\n",
    "                save_frame(output_image_path, frame)\n",
    "                break\n",
    "            \n",
    "            frame_number += frames_to_sample\n",
    "            if frame_number >= frame_count:\n",
    "                cap.release()\n",
    "                return\n",
    "\n",
    "        adjusted_bbox, adjusted_keypoints, exit_program = adjust_keypoints(output_image_path, bounding_box, predicted_keypoints, valid_frame_count + 1, 50)\n",
    "        if exit_program:\n",
    "            cap.release()\n",
    "            os.remove(output_image_path)  # Remove the image if exiting\n",
    "            return\n",
    "        if adjusted_bbox is None and adjusted_keypoints is None:\n",
    "            os.remove(output_image_path)  # Remove the image if skipping\n",
    "            frame_number += frames_to_sample\n",
    "            continue\n",
    "\n",
    "        formatted_output = format_output(adjusted_bbox, adjusted_keypoints, result.orig_shape)\n",
    "        save_output(output_image_path, formatted_output, video_name)\n",
    "        valid_frame_count += 1\n",
    "        frame_number += frames_to_sample\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Example usage\n",
    "main('input_videos/riga.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def create_split_directories(base_dir):\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        for folder in ['images', 'labels']:\n",
    "            path = os.path.join(base_dir, split, folder)\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "def copy_existing_split(existing_dir, output_dir, split):\n",
    "    images_dir = os.path.join(existing_dir, split, 'images')\n",
    "    labels_dir = os.path.join(existing_dir, split, 'labels')\n",
    "\n",
    "    output_images_dir = os.path.join(output_dir, split, 'images')\n",
    "    output_labels_dir = os.path.join(output_dir, split, 'labels')\n",
    "\n",
    "    for image in os.listdir(images_dir):\n",
    "        if image.endswith('.jpg'):\n",
    "            image_path = os.path.join(images_dir, image)\n",
    "            label_path = os.path.join(labels_dir, os.path.splitext(image)[0] + '.txt')\n",
    "            \n",
    "            shutil.copy(image_path, os.path.join(output_images_dir, image))\n",
    "            shutil.copy(label_path, os.path.join(output_labels_dir, os.path.splitext(image)[0] + '.txt'))\n",
    "\n",
    "def split_and_copy_new_data(new_data_dir, output_dir, train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1):\n",
    "    images_dir = os.path.join(new_data_dir, 'images')\n",
    "    labels_dir = os.path.join(new_data_dir, 'labels')\n",
    "\n",
    "    images = [f for f in os.listdir(images_dir) if f.endswith('.jpg')]\n",
    "    random.shuffle(images)\n",
    "\n",
    "    train_count = int(len(images) * train_ratio)\n",
    "    valid_count = int(len(images) * valid_ratio)\n",
    "\n",
    "    for i, image in enumerate(images):\n",
    "        if i < train_count:\n",
    "            split = 'train'\n",
    "        elif i < train_count + valid_count:\n",
    "            split = 'valid'\n",
    "        else:\n",
    "            split = 'test'\n",
    "        \n",
    "        image_path = os.path.join(images_dir, image)\n",
    "        label_path = os.path.join(labels_dir, os.path.splitext(image)[0] + '.txt')\n",
    "        \n",
    "        shutil.copy(image_path, os.path.join(output_dir, split, 'images', image))\n",
    "        shutil.copy(label_path, os.path.join(output_dir, split, 'labels', os.path.splitext(image)[0] + '.txt'))\n",
    "\n",
    "def merge_datasets(old_data_dir, new_data_dir, output_dir):\n",
    "    create_split_directories(output_dir)\n",
    "\n",
    "    # Copy existing splits from old data\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        copy_existing_split(old_data_dir, output_dir, split)\n",
    "\n",
    "    # Split and copy new data\n",
    "    split_and_copy_new_data(new_data_dir, output_dir)\n",
    "\n",
    "# Example usage\n",
    "output_dir = 'splitted_dataset'\n",
    "merge_datasets('old_data', 'dataset_maker', output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce GTX 1080\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # This should print True if a GPU is available\n",
    "print(torch.cuda.device_count())  # This will print the number of GPUs available\n",
    "print(torch.cuda.get_device_name(0))  # This will print the name of the GPU (if available)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.36 available  Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=pose, mode=train, model=models/Field_Key_Points.pt, data=data.yaml, epochs=2, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda:0, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=C:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\runs\\pose\\train2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  3    279808  ultralytics.nn.modules.block.C2f             [128, 128, 3, True]           \n",
      "  3                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  4                  -1  6   2101248  ultralytics.nn.modules.block.C2f             [256, 256, 6, True]           \n",
      "  5                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  6                  -1  6   8396800  ultralytics.nn.modules.block.C2f             [512, 512, 6, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  3   4461568  ultralytics.nn.modules.block.C2f             [512, 512, 3, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  3   1247744  ultralytics.nn.modules.block.C2f             [768, 256, 3]                 \n",
      " 16                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  3   4592640  ultralytics.nn.modules.block.C2f             [768, 512, 3]                 \n",
      " 19                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   4723712  ultralytics.nn.modules.block.C2f             [1024, 512, 3]                \n",
      " 22        [15, 18, 21]  1   6967411  ultralytics.nn.modules.head.Pose             [1, [32, 3], [256, 512, 512]] \n",
      "YOLOv8l-pose summary: 390 layers, 45014451 parameters, 45014435 gradients, 171.6 GFLOPs\n",
      "\n",
      "Transferred 637/637 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir C:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\runs\\pose\\train2', view at http://localhost:6006/\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\splitted_dataset\\train\\labels... 281 images, 0 backgrounds, 0 corrupt: 100%|| 281/281 [00:01<00:00, 210.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\splitted_dataset\\train\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\splitted_dataset\\valid\\labels... 47 images, 0 backgrounds, 0 corrupt: 100%|| 47/47 [00:00<00:00, 114.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\splitted_dataset\\valid\\labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to C:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\runs\\pose\\train2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 103 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\runs\\pose\\train2\u001b[0m\n",
      "Starting training for 2 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]c:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\env\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "        1/2      10.7G     0.3701      5.779     0.3268     0.3536     0.9924         44        640:  94%|| 17/18 [13:05<00:49, 49.34s/it]c:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "        1/2      10.7G     0.3687      5.756     0.3285     0.3609     0.9925         18        640: 100%|| 18/18 [13:21<00:00, 44.53s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Pose(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:59<00:00, 30.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         47         47      0.952      0.979      0.993      0.915      0.407      0.404      0.257     0.0568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss  pose_loss  kobj_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/2      10.7G      0.356      5.393      0.324     0.3322     0.9484         41        640:  11%|         | 2/18 [01:17<10:18, 38.64s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/Field_Key_Points.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels/Field_Key_Points_Trained.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\env\\Lib\\site-packages\\ultralytics\\engine\\model.py:673\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    670\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\env\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:199\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\env\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:371\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[0;32m    370\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[1;32m--> 371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    373\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n",
      "File \u001b[1;32mc:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\env\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:88\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;124;03mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\env\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:267\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[1;34m(self, batch, preds)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[0;32m    266\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Dokutsu\\Documents\\Project\\usg\\football_analysis\\env\\Lib\\site-packages\\ultralytics\\utils\\loss.py:461\u001b[0m, in \u001b[0;36mv8PoseLoss.__call__\u001b[1;34m(self, preds, batch)\u001b[0m\n\u001b[0;32m    458\u001b[0m pred_kpts \u001b[38;5;241m=\u001b[39m pred_kpts\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    460\u001b[0m dtype \u001b[38;5;241m=\u001b[39m pred_scores\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m--> 461\u001b[0m imgsz \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# image size (h,w)\u001b[39;00m\n\u001b[0;32m    462\u001b[0m anchor_points, stride_tensor \u001b[38;5;241m=\u001b[39m make_anchors(feats, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m    464\u001b[0m \u001b[38;5;66;03m# Targets\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('models/Field_Key_Points.pt').to('cuda')\n",
    "\n",
    "# Training\n",
    "model.train(data='data.yaml', epochs=2, imgsz=640)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('models/Field_Key_Points_Trained.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_formatted_output(image_path, formatted_output):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    if image is None:\n",
    "        print(f\"Error: Failed to load image at {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # Extract bounding box and keypoints from the formatted output\n",
    "    class_id = formatted_output[0]\n",
    "    bbox = formatted_output[1:5]\n",
    "    keypoints = formatted_output[5:]\n",
    "    \n",
    "    # Draw the bounding box\n",
    "    x_center, y_center, bbox_width, bbox_height = bbox\n",
    "    h, w, _ = image.shape\n",
    "    x_center, y_center = int(x_center * w), int(y_center * h)\n",
    "    bbox_width, bbox_height = int(bbox_width * w), int(bbox_height * h)\n",
    "    x1, y1 = x_center - bbox_width // 2, y_center - bbox_height // 2\n",
    "    x2, y2 = x_center + bbox_width // 2, y_center + bbox_height // 2\n",
    "    \n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    # Draw the keypoints\n",
    "    for i in range(0, len(keypoints), 3):\n",
    "        if i + 2 >= len(keypoints):\n",
    "            break  # Avoid index out of range error\n",
    "        x, y, vis = keypoints[i], keypoints[i+1], keypoints[i+2]\n",
    "        x, y = int(x * w), int(y * h)\n",
    "        if vis == 2:\n",
    "            cv2.circle(image, (x, y), 5, (0, 0, 255), -1)\n",
    "    \n",
    "    # Display the image\n",
    "    cv2.imshow('Annotated Image', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "output_image_path = 'dataset_maker/frame_250.jpg'\n",
    "\n",
    "\n",
    "# Draw the formatted output on the image\n",
    "draw_formatted_output(output_image_path, formatted_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
